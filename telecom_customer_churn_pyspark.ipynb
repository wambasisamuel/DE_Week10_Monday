{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1HW8GjoS7AN"
      },
      "source": [
        "# Telecom Customer Churn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisite"
      ],
      "metadata": {
        "id": "KfRrN2kwBCrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0R8W3x2BFzy",
        "outputId": "cdb74bef-3b3c-4c81-ab63-d7d25f2f694f"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLG2VTrnTvYL"
      },
      "source": [
        "## 1. Defining the Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XecOwPNorl2W"
      },
      "source": [
        "### a) Data Analysis Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZykmRrbGd13"
      },
      "source": [
        "Can we predict whether a customer will leave a telecom operator?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4wfHZwQrs-t"
      },
      "source": [
        "### b) Metric for Success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQcoaLFGjkH"
      },
      "source": [
        "The model should achieve a minimum accuracy of 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9BPYqunry97"
      },
      "source": [
        "### c) Understanding the context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jeqV9VPG1tg"
      },
      "source": [
        "Customer churn is a significant challenge in the telecom industry. Identifying customers who are likely to churn is crucial for implementing proactive measures to retain them.\n",
        "\n",
        "In this project, we leverage the distributed computing capabilities of PySpark to develop a machine learning model using PySpark that accurately predicts customer churn in a telecom company. The model should achieve a minimum accuracy of 0.8, enabling the company to proactively identify and retain customers at risk of leaving.\n",
        "\n",
        "By effectively predicting churn, the company can implement targeted retention strategies, reduce customer attrition, and improve overall business performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KMRBJ7zr9HD"
      },
      "source": [
        "### d) Experimental Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG0dB0_PG5_i"
      },
      "source": [
        "1. Data Importation\n",
        "2. Data Exploration\n",
        "3. Data Cleaning\n",
        "4. Data Preparation\n",
        "5. Data Modeling (Using Decision Trees, Random Forest and Logistic Regression)\n",
        "6. Model Evaluation\n",
        "7. Hyparameter Tuning\n",
        "8. Findings and Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSGyg6kWsBUl"
      },
      "source": [
        "### e) Data Relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UogUWzkTHJ9D"
      },
      "source": [
        "The given data set is relevant in answering the research question. The project includes relevant features such as customer demographics, usage patterns, service plans, call details, customer complaints, and churn status."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNbvIvnT7ep"
      },
      "source": [
        "## 2. Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STUSuuyTTBgx"
      },
      "source": [
        "# Importing our libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "# Disable warnings  when running cells\n",
        "import sys\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "       warnings.simplefilter(\"ignore\")\n",
        "\n"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJn2KjW-WMlG"
      },
      "source": [
        "# Load the data below\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"TelecomChurnPrediction\").getOrCreate()\n",
        "\n",
        "# Dataset url\n",
        "url = \"https://raw.githubusercontent.com/wambasisamuel/DE_Week10_Monday/main/telecom_dataset.csv\"\n",
        "\n",
        "# Download and load the dataset\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"telecom_dataset.csv\"), header=True, inferSchema= True)"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7168_y8EhZg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531af59e-a170-4d84-e1c8-9d9b97a82a45"
      },
      "source": [
        "# Preview the dataset: check first 5 rows of data\n",
        "df.show(5)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+--------------+--------------+------------+-----+\n",
            "|CustomerID|Gender|Age|      Contract|MonthlyCharges|TotalCharges|Churn|\n",
            "+----------+------+---+--------------+--------------+------------+-----+\n",
            "|         1|Female| 25|Month-to-Month|          65.7|       156.5|   No|\n",
            "|         2|  Male| 37|      One Year|          89.0|      2356.8|   No|\n",
            "|         3|  Male| 52|      Two Year|         115.5|      5408.6|   No|\n",
            "|         4|Female| 30|Month-to-Month|          75.9|       129.4|  Yes|\n",
            "|         5|  Male| 45|      One Year|          98.2|      3142.0|   No|\n",
            "+----------+------+---+--------------+--------------+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3sfP6y2hgIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bbbb3f-0421-4116-a718-ce3983cd3ede"
      },
      "source": [
        "# Check number of rows and columns\n",
        "# A custom-defined function that returns dataframe shape\n",
        "import pyspark\n",
        "def df_shape(dataFrame):\n",
        "    return (dataFrame.count(), len(dataFrame.columns))\n",
        "\n",
        "pyspark.sql.dataframe.DataFrame.shape = df_shape\n",
        "\n",
        "df.shape()"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AovarG6Bhk4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5142614-5984-4371-d6e4-30a71e754204"
      },
      "source": [
        "# Checking datatypes\n",
        "df.dtypes\n",
        "# OR df.schema.fields"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('CustomerID', 'int'),\n",
              " ('Gender', 'string'),\n",
              " ('Age', 'int'),\n",
              " ('Contract', 'string'),\n",
              " ('MonthlyCharges', 'double'),\n",
              " ('TotalCharges', 'double'),\n",
              " ('Churn', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKNcQTasmCgA"
      },
      "source": [
        "Observations:\n",
        "\n",
        "*   The are 20 observations in the dataset.\n",
        "*   The dataset has 7 features.\n",
        "*   There are 3 categorical features\n",
        "*   There are 4 numerical features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckfufNrcUHeH"
      },
      "source": [
        "## 3. External Data Source Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L4sl_0WXlbg"
      },
      "source": [
        "The provided dataset has enough features to help in developing a machine learning model that can predict customer churn from a telecom operator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlBMxEDBUc9B"
      },
      "source": [
        "## 4. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Standardisation"
      ],
      "metadata": {
        "id": "3R12ITECzhLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardise column names\n",
        "# change column names to lowercase\n",
        "df = df.toDF(*[col.lower() for col in df.columns])\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcHQpS8xzrDF",
        "outputId": "c9dbca63-0910-45e3-abac-4d4e53174e64"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['customerid',\n",
              " 'gender',\n",
              " 'age',\n",
              " 'contract',\n",
              " 'monthlycharges',\n",
              " 'totalcharges',\n",
              " 'churn']"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNtWOlYAkcO_"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Irrelevant Data"
      ],
      "metadata": {
        "id": "HtqhzVl5NT9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The customerid column has no use in churn prediction\n",
        "df = df.drop(\"customerid\")"
      ],
      "metadata": {
        "id": "L-3Mpe8ENVQo"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate data"
      ],
      "metadata": {
        "id": "ICn1Irlb5LWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the total duplicate records\n",
        "\"\"\"\n",
        "groupBy() all the columns and count()\n",
        "then select the sum of the counts for the rows where the count is greater than 1\n",
        "\"\"\"\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "df.groupBy(df.columns) \\\n",
        "    .count() \\\n",
        "    .where(f.col('count') > 1) \\\n",
        "    .select(f.sum('count')) \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvpfHp1x7gbT",
        "outputId": "e1f7e703-3f90-4c1a-dcc1-085a77369d44"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|      null|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Data"
      ],
      "metadata": {
        "id": "nocqWsTV5UcU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYxrLT0GiQSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbfba66-b46f-4cda-ac6a-5c0efe1fe475"
      },
      "source": [
        "# Checking missing entries of all the variables\n",
        "\n",
        "# Find count for empty, None, Null, Nan with string literals.\n",
        "from pyspark.sql.functions import col,isnan,when,count\n",
        "missing_counts = df.select([count(when(col(c).contains('None') | \\\n",
        "                            col(c).contains('NULL') | \\\n",
        "                            (col(c) == '' ) | \\\n",
        "                            col(c).isNull() | \\\n",
        "                            isnan(c), c\n",
        "                           )).alias(c)\n",
        "                    for c in df.columns])\n",
        "missing_counts.show()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------+--------------+------------+-----+\n",
            "|gender|age|contract|monthlycharges|totalcharges|churn|\n",
            "+------+---+--------+--------------+------------+-----+\n",
            "|     0|  0|       0|             0|           0|    0|\n",
            "+------+---+--------+--------------+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has no duplicate records nor missing values."
      ],
      "metadata": {
        "id": "_UpLEd86M0b-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTbdjSrhVIiT"
      },
      "source": [
        "## 5. Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing"
      ],
      "metadata": {
        "id": "xI72zByGKMNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating new features from the existing dataset"
      ],
      "metadata": {
        "id": "A2GLeqmprQcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate new features\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "# customer tenure column\n",
        "#df = df.withColumn(\"tenure_months\", (col(\"totalcharges\") / col(\"monthlycharges\")).cast(\"double\"))\n",
        "df = df.withColumn(\"tenure_months\", (f.round(col(\"totalcharges\") / col(\"monthlycharges\"), scale=2)).cast(\"double\"))"
      ],
      "metadata": {
        "id": "pR5iCyLIrPDL"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdHyc8WYHlAG"
      },
      "source": [
        "Encoding and feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Encode categorical features into numerical form\n",
        "categorical_cols = [\"gender\", \"contract\"]\n",
        "\n",
        "# Convert string columns to numerical categories\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\")\n",
        "    for col in [\"gender\", \"contract\", \"churn\"]\n",
        "]\n",
        "\n",
        "indexer_pipeline = Pipeline(stages=indexers)\n",
        "transformed_df = indexer_pipeline.fit(df).transform(df)\n",
        "\n",
        "# Cast the \"Target\" column to IntegerType\n",
        "#df = df.withColumn(\"churn\", col(\"churn\").cast(IntegerType()))\n",
        "\n",
        "# Set the feature columns\n",
        "featureCols = [\"gender_index\", \"contract_index\", \"monthlycharges\", \"totalcharges\", \"tenure_months\"]\n",
        "\n",
        "# Convert feature columns to numeric types\n",
        "for featureCol in featureCols:\n",
        "    transformed_df = transformed_df.withColumn(featureCol, col(featureCol).cast(\"double\"))\n",
        "\n",
        "# Vectorize the feature columns\n",
        "vecAssembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
        "feature_vector = vecAssembler.transform(transformed_df)\n",
        "\n",
        "feature_vector.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTQzAalQlyIJ",
        "outputId": "a78d994b-d1c7-4277-826d-3c6de3e5d3fc"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+\n",
            "|gender|age|      contract|monthlycharges|totalcharges|churn|tenure_months|gender_index|contract_index|churn_index|            features|\n",
            "+------+---+--------------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+\n",
            "|Female| 25|Month-to-Month|          65.7|       156.5|   No|         2.38|         1.0|           0.0|        0.0|[1.0,0.0,65.7,156...|\n",
            "|  Male| 37|      One Year|          89.0|      2356.8|   No|        26.48|         0.0|           1.0|        0.0|[0.0,1.0,89.0,235...|\n",
            "|  Male| 52|      Two Year|         115.5|      5408.6|   No|        46.83|         0.0|           2.0|        0.0|[0.0,2.0,115.5,54...|\n",
            "|Female| 30|Month-to-Month|          75.9|       129.4|  Yes|          1.7|         1.0|           0.0|        1.0|[1.0,0.0,75.9,129...|\n",
            "|  Male| 45|      One Year|          98.2|      3142.0|   No|         32.0|         0.0|           1.0|        0.0|[0.0,1.0,98.2,314...|\n",
            "+------+---+--------------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "categorical_cols = [\"gender\", \"contract\"]\n",
        "#numeric_cols = [\"age\", \"monthlycharges\", \"totalcharges\",\"tenure_months\"]\n",
        "numeric_cols = [col for col, dtype in df.dtypes if dtype != 'string']\n",
        "\n",
        "stringIndexer = StringIndexer(inputCols=[cols for cols in categorical_cols],\n",
        "                              outputCols=[cols + \"_index\" for cols in categorical_cols])\n",
        "\n",
        "encoder = OneHotEncoder(inputCols=[cols + \"_index\" for cols in categorical_cols],\n",
        "                                 outputCols=[cols + \"_classVec\" for cols in categorical_cols])\n",
        "\n",
        "stages = []\n",
        "\n",
        "stages += [stringIndexer, encoder]\n",
        "\n",
        "label_string_id = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
        "stages += [label_string_id]\n",
        "\n",
        "# Feature scaling\n",
        "assembler_inputs = [cols + \"_classVec\" for cols in categorical_cols] + numeric_cols\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"unscaled_features\")\n",
        "stages += [assembler]\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
        "stages += [scaler]\n",
        "\n",
        "# Label the target variable (churn): Map 'Y'  to 1 and 'N' to 0\n",
        "#df = df.withColumn(\"label\", (df[\"churn\"] == \"Yes\").cast(\"integer\"))\n",
        "\n",
        "\n",
        "# Run Data Through Pipeline\n",
        "pipeline = Pipeline().setStages(stages)\n",
        "pipeline_model = pipeline.fit(df)\n",
        "pipelined_df = pipeline_model.transform(df)\n",
        "\n",
        "pipelined_df.dtypes\n",
        "pipelined_df.show(5)\n",
        "#df.show(5)"
      ],
      "metadata": {
        "id": "IHJcGXF-Kg8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f15c11-4192-46bd-e35d-c4cfed6f5ab6"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------------+--------------+------------+-----+-------------+------------+--------------+---------------+-----------------+-----+--------------------+--------------------+\n",
            "|gender|age|      contract|monthlycharges|totalcharges|churn|tenure_months|gender_index|contract_index|gender_classVec|contract_classVec|label|   unscaled_features|            features|\n",
            "+------+---+--------------+--------------+------------+-----+-------------+------------+--------------+---------------+-----------------+-----+--------------------+--------------------+\n",
            "|Female| 25|Month-to-Month|          65.7|       156.5|   No|         2.38|         1.0|           0.0|      (1,[],[])|    (2,[0],[1.0])|  0.0|[0.0,1.0,0.0,25.0...|[0.0,1.9895560643...|\n",
            "|  Male| 37|      One Year|          89.0|      2356.8|   No|        26.48|         0.0|           1.0|  (1,[0],[1.0])|    (2,[1],[1.0])|  0.0|[1.0,0.0,1.0,37.0...|[1.95917937881752...|\n",
            "|  Male| 52|      Two Year|         115.5|      5408.6|   No|        46.83|         0.0|           2.0|  (1,[0],[1.0])|        (2,[],[])|  0.0|[1.0,0.0,0.0,52.0...|[1.95917937881752...|\n",
            "|Female| 30|Month-to-Month|          75.9|       129.4|  Yes|          1.7|         1.0|           0.0|      (1,[],[])|    (2,[0],[1.0])|  1.0|[0.0,1.0,0.0,30.0...|[0.0,1.9895560643...|\n",
            "|  Male| 45|      One Year|          98.2|      3142.0|   No|         32.0|         0.0|           1.0|  (1,[0],[1.0])|    (2,[1],[1.0])|  0.0|[1.0,0.0,1.0,45.0...|[1.95917937881752...|\n",
            "+------+---+--------------+--------------+------------+-----+-------------+------------+--------------+---------------+-----------------+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Selection"
      ],
      "metadata": {
        "id": "_SEtQQrb9Iul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting the dataset"
      ],
      "metadata": {
        "id": "MytG_1UONsl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, test_data) = feature_vector.randomSplit([0.75, 0.25], seed=1337)\n",
        "# 25% - testing, 75% - training."
      ],
      "metadata": {
        "id": "BsVwYYdILwPm"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "x4t0l-RT9L8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.ml.regression import LogisticRegression\n",
        "\n",
        "# Create Logistic Regression model\n",
        "#lr_regressor = LogisticRegression(labelCol=\"churn_index\", featuresCol=\"features\", predictionCol=\"predicted_churn\")\n",
        "lr_regressor = LogisticRegression(labelCol=\"churn_index\", featuresCol=\"features\", predictionCol=\"predicted_churn\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Train the model\n",
        "model = lr_regressor.fit(train_data)\n",
        "\n",
        "# Test the model\n",
        "lr_predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "lr_predictions.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5N10BKE79_K",
        "outputId": "7189403b-77c2-413a-c850-d118ffce0b93"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+\n",
            "|gender|age|contract|monthlycharges|totalcharges|churn|tenure_months|gender_index|contract_index|churn_index|            features|       rawPrediction|         probability|predicted_churn|\n",
            "+------+---+--------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+\n",
            "|Female| 55|Two Year|          99.9|      6541.5|   No|        65.48|         1.0|           2.0|        0.0|[1.0,2.0,99.9,654...|[1.68662779498050...|[0.58698954980392...|            0.0|\n",
            "|  Male| 41|Two Year|          96.5|      4188.1|   No|         43.4|         0.0|           2.0|        0.0|[0.0,2.0,96.5,418...|[1.68662779498050...|[0.58698954980392...|            0.0|\n",
            "|  Male| 48|One Year|         101.8|      5149.6|  Yes|        50.59|         0.0|           1.0|        1.0|[0.0,1.0,101.8,51...|[1.68662779498050...|[0.58698954980392...|            0.0|\n",
            "+------+---+--------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Classification"
      ],
      "metadata": {
        "id": "7LPqoBmF-CZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(labelCol=\"churn_index\", featuresCol=\"features\",numTrees=4, predictionCol=\"predicted_churn\")\n",
        "\n",
        "# Create a Pipeline\n",
        "pipeline = Pipeline(stages=[rf_classifier])\n",
        "\n",
        "# Fit the Model\n",
        "model = pipeline.fit(feature_vector)\n",
        "\n",
        "# Make predictions\n",
        "rf_predictions = model.transform(test_data)\n",
        "rf_predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_j9unRY4OpJ",
        "outputId": "12423dbb-7a80-41fd-d9e1-7287d8027749"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+--------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+-------------+---------------+---------------+\n",
            "|gender|age|contract|monthlycharges|totalcharges|churn|tenure_months|gender_index|contract_index|churn_index|            features|rawPrediction|    probability|predicted_churn|\n",
            "+------+---+--------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+-------------+---------------+---------------+\n",
            "|Female| 55|Two Year|          99.9|      6541.5|   No|        65.48|         1.0|           2.0|        0.0|[1.0,2.0,99.9,654...|[2.0,2.0,0.0]|  [0.5,0.5,0.0]|            0.0|\n",
            "|  Male| 41|Two Year|          96.5|      4188.1|   No|         43.4|         0.0|           2.0|        0.0|[0.0,2.0,96.5,418...|[3.0,1.0,0.0]|[0.75,0.25,0.0]|            0.0|\n",
            "|  Male| 48|One Year|         101.8|      5149.6|  Yes|        50.59|         0.0|           1.0|        1.0|[0.0,1.0,101.8,51...|[1.0,3.0,0.0]|[0.25,0.75,0.0]|            1.0|\n",
            "+------+---+--------+--------------+------------+-----+-------------+------------+--------------+-----------+--------------------+-------------+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Evaluation"
      ],
      "metadata": {
        "id": "4XvcPg3p-OWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Evaluation metrics\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"predicted_churn\",predictionCol=\"predicted_churn\")\n",
        "\n",
        "\n",
        "accuracy_lr = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"accuracy\"})\n",
        "precision_lr = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_lr = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "f1_lr = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"f1\"})\n",
        "\n",
        "\n",
        "accuracy_rf = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"accuracy\"})\n",
        "precision_rf = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_rf = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "f1_rf = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"f1\"})\n",
        "\n",
        "print(\"Logistic Regression accuracy:\", accuracy_lr)\n",
        "print(\"Logistic Regression Precision:\", precision_lr)\n",
        "print(\"Logistic Regression recall:\", recall_lr)\n",
        "print(\"Logistic Regression F1-score:\", f1_lr)\n",
        "\n",
        "print(\"\\nRandom Forest accuracy:\", accuracy_rf)\n",
        "print(\"Random Forest Precision:\", accuracy_rf)\n",
        "print(\"Random Forest recall:\", accuracy_rf)\n",
        "print(\"Random Forest F1-score:\", accuracy_rf)"
      ],
      "metadata": {
        "id": "-ggAeMfg83a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0fe25b8-6fa2-44c8-d2fb-415dfe6497c6"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy: 1.0\n",
            "Logistic Regression Precision: 1.0\n",
            "Logistic Regression recall: 1.0\n",
            "Logistic Regression F1-score: 1.0\n",
            "\n",
            "Random Forest accuracy: 1.0\n",
            "Random Forest Precision: 1.0\n",
            "Random Forest recall: 1.0\n",
            "Random Forest F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpmGSOOqIMrt"
      },
      "source": [
        "## 6. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDn2whdV0Zcg"
      },
      "source": [
        "#### Challenges\n",
        "\n",
        "- The dataset provided limited information, which required careful feature engineering and selection to improve the model's performance.\n",
        "\n",
        "- churned customers were significantly fewer than the non-churned customers, leading to imbalanced classes.\n",
        "\n",
        "#### Findings\n",
        "\n",
        "* The two selected models are good in predicting customer churn\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}